{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy\n",
    "import scipy.odr as odr\n",
    "from scipy import stats\n",
    "from statistics import mean\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from Common_Constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_timeloc_func(df, start_time, end_time, df_time_col='time'):\n",
    "    \"\"\"\n",
    "    Crops DF's into specified time intervals\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : object\n",
    "        Pass in your DF\n",
    "    start_time : str\n",
    "        Start time as a string in datetime format (EX '2020-01-22 00:00:00')\n",
    "    end_time : str\n",
    "        End time as a string in datetime format (EX '2020-08-28 05:00:00')\n",
    "    df_time_col : str\n",
    "        Time column header name as string Default('time')\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        DF object cropped by specified start and end time bounds\n",
    "    \"\"\"\n",
    "    start = dt.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "    end = dt.datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "    df = df.loc[(df[df_time_col] >= start) & (df[df_time_col] < end)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_denver_tz_func(df, time_column_header='time'):\n",
    "    \"\"\"\n",
    "    Converts DF['time'] column data timezone from UTC to local(Denver) time\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : object\n",
    "        Pass in your DF\n",
    "    time_column_header: str\n",
    "        name of time column as string (default 'time')\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        DF object with time column converted to local (Denver) timezone\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df[time_column_header] = df[time_column_header].dt.tz_localize('UTC').dt.tz_convert('America/Denver')\n",
    "        df[time_column_header] = df[time_column_header].dt.tz_localize(None)\n",
    "        return df\n",
    "    except:\n",
    "        raise Exception('to_denver_tz_func takes two arguments (df, time column header (default \"time\"))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_title_func(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Convert the start and end times time to a title format\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_time : str\n",
    "        Start time as a string in datetime format (EX '2020-01-22 00:00:00')\n",
    "    end_time : str\n",
    "        End time as a string in datetime format (EX '2020-08-28 05:00:00')\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A title string in (m/d/y-m/d/y) format\n",
    "    \"\"\"\n",
    "    start = dt.datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "    end = dt.datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "    time_title = str(start.month) + '/' + str(start.day) + '/' + str(start.year)[-2:] + '-' + str(end.month) + '/' + \\\n",
    "                 str(end.day) + '/' + str(end.year)[-2:]\n",
    "    return time_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_title_string_func(sites_name_list):\n",
    "    \"\"\"\n",
    "    Convert a site list to a title string format\n",
    "    Parameters\n",
    "    ----------\n",
    "    sites_name_list : str\n",
    "        A list of site abbreviations:\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A title string in (m/d/y-m/d/y) format\n",
    "    \"\"\"\n",
    "    if len(sites_name_list) > 1:\n",
    "        title_string = ''\n",
    "        for site_name in sites_name_list:\n",
    "            if site_name != sites_name_list[-1]:\n",
    "                title_string += site_name + ', '\n",
    "            else:\n",
    "                title_string += '& ' + site_name\n",
    "        return title_string\n",
    "    else:\n",
    "        return sites_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_binning_func(df, bin_time_interval):\n",
    "    \"\"\"\n",
    "    Converts time column to binned frequency\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : object\n",
    "        Your data frame must have 'time' column in datetime format\n",
    "    bin_time_interval : str\n",
    "        datetime binging string (EX: 'month', 'week', 'day', 'year', or 'all time')\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        DF with binned time column on specified time interval\n",
    "    \"\"\"\n",
    "    if bin_time_interval == 'year':  # setting up bin intervals ex every month, every year, ect...\n",
    "        df['time'] = df['time'].dt.year\n",
    "    elif bin_time_interval == 'month':\n",
    "        df['time'] = df['time'].dt.month\n",
    "        df.sort_values(by='time', inplace=True)\n",
    "        df['time'].replace(\n",
    "            {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\", 7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\",\n",
    "             11: \"Nov\", 12: \"Dec\"}, inplace=True)\n",
    "    elif bin_time_interval == 'week':\n",
    "        df['time'] = df['time'].dt.week\n",
    "    elif bin_time_interval == 'day':\n",
    "        df['time'] = df['time'].dt.day\n",
    "    elif bin_time_interval == 'all time':\n",
    "        df['time'] = ''\n",
    "    else:\n",
    "        try:\n",
    "            1 + bin_time_interval\n",
    "        except:\n",
    "            print('Error! bin_time_interval not recognized')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_with_site_func(sites_list, df_list):\n",
    "    \"\"\"\n",
    "    Concatenates multiple DF's into one and adds a column (df['site]) with site names as values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    sites_list : list of str\n",
    "        List of sites there is data for\n",
    "    df_list : list of object\n",
    "        list of site dataframes\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        Single combine df with new site column\n",
    "    \"\"\"\n",
    "    for i in range(len(sites_list)):\n",
    "        df_list[i]['site'] = sites_list[i]\n",
    "        # creating a site column for plotting\n",
    "    data = pd.concat(df_list)\n",
    "    # combing df list into single df for plotting\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarters_and_years_func(start_time, end_time):\n",
    "    \"\"\"\n",
    "    get relevant quarters and years as a list of strings from user supplied start_time and end_time stings.\n",
    "    Parameters\n",
    "    __________\n",
    "    start_time : str\n",
    "        Start time as a string in datetime format (EX '2020-01-22 00:00:00')\n",
    "    end_time : str\n",
    "        End time as a string in datetime format (EX '2020-08-28 05:00:00')\n",
    "    Returns\n",
    "    _______\n",
    "    list of str\n",
    "        list of strings with all years and quarters between the start and end times. The strings look like this:\n",
    "        ('2020_Q2')\n",
    "    \"\"\"\n",
    "    start = dt.datetime.strptime((start_time[:7]), '%Y-%m')\n",
    "    end = dt.datetime.strptime((end_time[:7]), '%Y-%m')\n",
    "    num_months = (end.year - start.year) * 12 + (end.month - start.month) + 1\n",
    "    dates_list = [start + relativedelta(months=m) for m in range(num_months)]\n",
    "    quarters_list = []\n",
    "    for date in dates_list:\n",
    "        if date.month == 1 or date.month == 2 or date.month == 3:\n",
    "            quarters_list.append(str(date.year) + '_' + 'q1')\n",
    "        elif date.month == 4 or date.month == 5 or date.month == 6:\n",
    "            quarters_list.append(str(date.year) + '_' + 'q2')\n",
    "        elif date.month == 7 or date.month == 8 or date.month == 9:\n",
    "            quarters_list.append(str(date.year) + '_' + 'q3')\n",
    "        else:\n",
    "            quarters_list.append(str(date.year) + '_' + 'q4')\n",
    "    quarters_list = list(set(quarters_list))\n",
    "    return quarters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_path_generator_func(file_path, sites, species, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Constructs a list of relevant file_paths\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Your base file path to Boulder AIR CSV data (EX: 'E:\\IDAT')\n",
    "    sites : list of str\n",
    "        List of site(s) codes as strings (EX: ['LUR', 'BSE'])\n",
    "    species : str\n",
    "        String of species as it appears in the data column (EX: 'ethane')\n",
    "    start_time : str\n",
    "        Start time as a string in datetime format (EX '2020-01-22 00:00:00')\n",
    "    end_time : str\n",
    "        End time as a string in datetime format (EX '2020-08-28 05:00:00')\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of file path names to be imported\n",
    "    \"\"\"\n",
    "    path_species = None\n",
    "    if species in MET_LIST or species == 'met':\n",
    "        path_species = 'met'\n",
    "    elif species == 'ch4' or species == 'co2' or species == 'co2_ppm' or species == 'co' or species == 'co2':\n",
    "        path_species = 'ch4'\n",
    "    elif species == 'o3':\n",
    "        path_species = 'ozone'\n",
    "    elif species == 'pm2_5' or species == 'pm10':\n",
    "        path_species = 'pm'\n",
    "    elif species in VOC_LIST:\n",
    "        path_species = 'voc'\n",
    "    elif species == 'no' or species == 'nox':\n",
    "        path_species = 'nox'\n",
    "    elif species == 'h2s' or species == 'so2':\n",
    "        path_species = 'met'\n",
    "    elif species == 'radon':\n",
    "        path_species = 'radon'\n",
    "    else:\n",
    "        print('species not recognized')\n",
    "    if path_species == 'nox':\n",
    "        file_paths = []\n",
    "        for site in sites:\n",
    "            if site == 'BSE' or site == 'CCF' or site == 'ESF' or site == 'CCM' or site == 'LUR':\n",
    "                file_paths.append(file_path + '/' + site + '/' + 'met/')\n",
    "            else:\n",
    "                file_paths.append(file_path + '/' + site + '/' + path_species + '/')\n",
    "    else:\n",
    "        file_paths = [(file_path + '/' + site + '/' + path_species + '/') for site in sites]\n",
    "    all_files = []\n",
    "    for file_path in file_paths:\n",
    "        file = glob.glob(r'' + file_path + \"/*.csv\")\n",
    "        all_files.append(file)\n",
    "    for file in all_files:\n",
    "        if file == all_files[0]:\n",
    "            combined_files = file\n",
    "        else:\n",
    "            combined_files += file\n",
    "    quarters = get_quarters_and_years_func(start_time, end_time)\n",
    "    good_file_paths = []\n",
    "    for path in combined_files:\n",
    "        year_q_position = path.find('_20')\n",
    "        date_info = path[(year_q_position + 1): (year_q_position + 8)]\n",
    "        if date_info in quarters:\n",
    "            good_file_paths.append(path)\n",
    "    return good_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_file_path_generator_func(file_path, sites, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Constructs a list of relevant file_paths\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Your base file path to Boulder AIR CSV data (EX: 'E:\\IDAT')\n",
    "    sites : list of str\n",
    "        List of site(s) codes as strings (EX: ['LUR', 'BSE'])\n",
    "    species : str\n",
    "        String of species as it appears in the data column (EX: 'ethane')\n",
    "    start_time : str\n",
    "        Start time as a string in datetime format (EX '2020-01-22 00:00:00')\n",
    "    end_time : str\n",
    "        End time as a string in datetime format (EX '2020-08-28 05:00:00')\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of file path names to be imported\n",
    "    \"\"\"\n",
    "    wind_paths = []\n",
    "    for site in sites:\n",
    "        wind_paths.append(file_path + '//' + site + '//met')\n",
    "    all_files = []\n",
    "    for file_path in wind_paths:\n",
    "        file = glob.glob(r'' + file_path + \"/*.csv\")\n",
    "        all_files.append(file)\n",
    "    for file in all_files:\n",
    "        if file == all_files[0]:\n",
    "            combined_files = file\n",
    "        else:\n",
    "            combined_files += file\n",
    "    quarters = get_quarters_and_years_func(start_time, end_time)\n",
    "    good_file_paths = []\n",
    "    for path in combined_files:\n",
    "        year_q_position = path.find('_20')\n",
    "        date_info = path[(year_q_position + 1): (year_q_position + 8)]\n",
    "        if date_info in quarters:\n",
    "            good_file_paths.append(path)\n",
    "    return good_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_import_func(file_paths, sites, header_num=1):\n",
    "    \"\"\"\n",
    "    Constructs a list of relevant file_paths\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_paths : str\n",
    "        Your base file path to Boulder AIR CSV data (EX: 'E:\\IDAT')\n",
    "    sites : list of str\n",
    "        List of site(s) codes as strings (EX: ['LUR', 'BSE'])\n",
    "    header_num : int\n",
    "        String of species as it appears in the data column (EX: 'ethane')\n",
    "    Returns\n",
    "    -------\n",
    "    list of objects\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    active_site = sites[0]\n",
    "    df_list = []\n",
    "    data_list = []\n",
    "    for path in file_paths:\n",
    "        data = pd.read_csv(path, header=header_num)\n",
    "        data['time'] = pd.to_datetime(data['time'], unit='s')\n",
    "        data['time'] = data['time'].dt.round('1min')\n",
    "        if active_site in path and path != file_paths[-1]:\n",
    "            df_list.append(data)\n",
    "        elif active_site not in path and path != file_paths[-1]:\n",
    "            data_list.append(df_list)\n",
    "            df_list = []\n",
    "            df_list.append(data)\n",
    "            count += 1\n",
    "            active_site = sites[count]\n",
    "        elif active_site in path and path == file_paths[-1]:\n",
    "            df_list.append(data)\n",
    "            data_list.append(df_list)\n",
    "        else:\n",
    "            data_list.append(df_list)\n",
    "            df_list = []\n",
    "            df_list.append(data)\n",
    "            data_list.append(df_list)\n",
    "    combine_data_list = []\n",
    "    for lst in data_list:\n",
    "        # data = lst[0]\n",
    "        count = 0\n",
    "        data_list_1 = []\n",
    "        data_list_2 = []\n",
    "        two_sets_of_data = False\n",
    "        for df in lst:\n",
    "            if count == 0:\n",
    "                data_list_1.append(df)\n",
    "                count += 1\n",
    "            elif df.columns[1] in data_list_1[0].columns:\n",
    "                data_list_1.append(df)\n",
    "                count += 1\n",
    "            else:\n",
    "                two_sets_of_data = True\n",
    "                data_list_2.append(df)\n",
    "                count += 1\n",
    "        if len(data_list_1) == 1:\n",
    "            data1 = data_list_1[0]\n",
    "        else:\n",
    "            data1 = pd.concat(data_list_1)\n",
    "        if two_sets_of_data:\n",
    "            if len(data_list_2) == 1:\n",
    "                data2 = data_list_2[0]\n",
    "            else:\n",
    "                data2 = pd.concat(data_list_2)\n",
    "            data = pd.merge(data1, data2, on='time', how='outer')\n",
    "        else:\n",
    "            data = data1\n",
    "        data = data.sort_values(by=['time'])\n",
    "        combine_data_list.append(data)\n",
    "    return combine_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_wind_pairing_func(met_df,\n",
    "                          voc_df):\n",
    "    \"\"\"\n",
    "    This function is meant to pair met and VOC data together (can also add methane data if needed)\n",
    "    The function averages the met data over the 10 minute GC VOC sampling interval and places the data point in the\n",
    "    middle of the interval.  Wind direction data is averaged using a special method since it has a circular scale\n",
    "    (0 to 360).  Methane may also be included in the averaged interval if it is present in the met data.\n",
    "    Parameters\n",
    "    __________\n",
    "    met_df : object\n",
    "        dataframe containing met data\n",
    "    voc_df : object\n",
    "        dataframe containing VOC data\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        returns a dataframe with merged voc and met data\n",
    "    \"\"\"\n",
    "    # imput met df and voc df returns merged df with average windspeed and durection over voc sampaling interval\n",
    "    voc_df['time'] = voc_df['time'].dt.round('1min')\n",
    "    bool_list = met_df['time'].isin(voc_df['time'])\n",
    "    met_df['bool'] = bool_list\n",
    "    data = pd.merge(voc_df, met_df, on='time', how='outer')\n",
    "    data = data.set_index(['time'])  # sorting index by date\n",
    "    data = data.sort_index()\n",
    "    data = data.reset_index()\n",
    "    ind_bool = data.loc[data['bool'] == True]\n",
    "    goodind = ind_bool.index.tolist()\n",
    "    ch4_bool = False\n",
    "    reallygoodind = []\n",
    "    for i in goodind:  # getting index values within 9 mins of voc data colection\n",
    "        reallygoodind.append(i - 4)\n",
    "        reallygoodind.append(i - 3)\n",
    "        reallygoodind.append(i - 2)\n",
    "        reallygoodind.append(i - 1)\n",
    "        reallygoodind.append(i)\n",
    "        reallygoodind.append(i + 1)\n",
    "        reallygoodind.append(i + 2)\n",
    "        reallygoodind.append(i + 3)\n",
    "        reallygoodind.append(i + 4)\n",
    "    data['index'] = data.index\n",
    "    data2 = [data['index'].isin(reallygoodind)]\n",
    "    data['bool2'] = np.transpose(data2)\n",
    "    data = data.loc[data['bool2'] == True]\n",
    "    if 'wsp_avg_ms' in list(data.columns) or 'wdr_avg' in list(data.columns):\n",
    "        data = data.rename(columns={'wsp_avg_ms': 'wsp', 'wdr_avg': 'wdr'})\n",
    "    if 'ch4' in data.columns:\n",
    "        data['ch4'] = data['ch4'].astype(float)\n",
    "        ch4_bool = True\n",
    "    data['wdr'] = data['wdr'].astype(float)  # converting wdr and wsp into floating point values for later functions\n",
    "    data['wsp'] = data['wsp'].astype(float)\n",
    "    data['x'] = np.sin((data['wdr'] * np.pi / 180))  # converting wdr into east-west and north-south components\n",
    "    data['y'] = np.cos((data['wdr'] * np.pi / 180))\n",
    "    avg_wsp_list = []\n",
    "    avg_list = []\n",
    "    fixedind = []\n",
    "    last_ind = None\n",
    "    if ch4_bool is True:\n",
    "        avg_ch4_list = []\n",
    "        for ind, row in data.iterrows():\n",
    "            if ind - 1 == last_ind:\n",
    "                xlist.append(row['x'])\n",
    "                ylist.append(row['y'])\n",
    "                last_ind = ind\n",
    "                avgwsplist.append(row['wsp'])\n",
    "                avgch4list.append(row['ch4'])\n",
    "            elif ind == reallygoodind[0]:\n",
    "                avgch4list = []\n",
    "                avgwsplist = []\n",
    "                xlist = []\n",
    "                ylist = []\n",
    "                xlist.append(row['x'])\n",
    "                ylist.append(row['y'])\n",
    "                avgwsplist.append(row['wsp'])\n",
    "                avgch4list.append(row['ch4'])\n",
    "                last_ind = ind\n",
    "            else:\n",
    "                centerind = (last_ind - 4)\n",
    "                fixedind.append(centerind)\n",
    "                xtot = sum(xlist)\n",
    "                ytot = sum(ylist)\n",
    "                dev = (xtot / ytot)\n",
    "                ark = (np.arctan(dev))\n",
    "                avg = (ark * 57.2958)\n",
    "                if xtot > 0 and ytot > 0:  # if loop used to determin which quadrent wind direction data fit in\n",
    "                    avg_list.append(avg)\n",
    "                elif xtot > 0 and ytot < 0:\n",
    "                    avg2nd = (avg + 180)\n",
    "                    avg_list.append(avg2nd)\n",
    "                elif xtot < 0 and ytot < 0:\n",
    "                    avg3rd = (avg + 180)\n",
    "                    avg_list.append(avg3rd)\n",
    "                elif xtot < 0 and ytot > 0:\n",
    "                    avg4th = (avg + 360)\n",
    "                    avg_list.append(avg4th)\n",
    "                else:\n",
    "                    avg_list.append(np.nan)\n",
    "                wsp = mean(avgwsplist)\n",
    "                ch4 = mean(avgch4list)\n",
    "                avg_wsp_list.append(wsp)\n",
    "                avg_ch4_list.append(ch4)\n",
    "                xlist = []\n",
    "                ylist = []\n",
    "                avgwsplist = []\n",
    "                avgch4list = []\n",
    "                avgwsplist.append(row['wsp'])\n",
    "                avgch4list.append(row['ch4'])\n",
    "                xlist.append(row['x'])\n",
    "                ylist.append(row['y'])\n",
    "                last_ind = ind\n",
    "    else:\n",
    "        for ind, row in data.iterrows():\n",
    "            if ind - 1 == last_ind:\n",
    "                xlist.append(row['x'])\n",
    "                ylist.append(row['y'])\n",
    "                last_ind = ind\n",
    "                avgwsplist.append(row['wsp'])\n",
    "            elif ind == reallygoodind[0]:\n",
    "                avgwsplist = []\n",
    "                xlist = []\n",
    "                ylist = []\n",
    "                xlist.append(row['x'])\n",
    "                ylist.append(row['y'])\n",
    "                avgwsplist.append(row['wsp'])\n",
    "                last_ind = ind\n",
    "            else:\n",
    "                centerind = (last_ind - 4)\n",
    "                fixedind.append(centerind)\n",
    "                xtot = sum(xlist)\n",
    "                ytot = sum(ylist)\n",
    "                dev = (xtot / ytot)\n",
    "                ark = (np.arctan(dev))\n",
    "                avg = (ark * 57.2958)\n",
    "                if xtot > 0 and ytot > 0:  # if loop used to determin which quadrent wind direction data fit in\n",
    "                    avg_list.append(avg)\n",
    "                elif xtot > 0 and ytot < 0:\n",
    "                    avg2nd = (avg + 180)\n",
    "                    avg_list.append(avg2nd)\n",
    "                elif xtot < 0 and ytot < 0:\n",
    "                    avg3rd = (avg + 180)\n",
    "                    avg_list.append(avg3rd)\n",
    "                elif xtot < 0 and ytot > 0:\n",
    "                    avg4th = (avg + 360)\n",
    "                    avg_list.append(avg4th)\n",
    "                else:\n",
    "                    avg_list.append(np.nan)\n",
    "                wsp = mean(avgwsplist)\n",
    "                avg_wsp_list.append(wsp)\n",
    "                xlist = []\n",
    "                ylist = []\n",
    "                avgwsplist = []\n",
    "                avgwsplist.append(row['wsp'])\n",
    "                xlist.append(row['x'])\n",
    "                ylist.append(row['y'])\n",
    "                last_ind = ind\n",
    "    avgdata = pd.DataFrame(fixedind, columns=['index'])\n",
    "    avgdata['avg_wdr'] = avg_list\n",
    "    avgdata['avg_wsp'] = avg_wsp_list\n",
    "    if ch4_bool is True:\n",
    "        avgdata['avg_ch4'] = avg_ch4_list\n",
    "    data = pd.merge(data, avgdata, on=['index'], how='outer')  # merging data frames\n",
    "    check = data\n",
    "    data = data.loc[data['bool'] == True]\n",
    "    if ch4_bool is True:\n",
    "        data = data.drop(\n",
    "            columns=['x', 'y', 'wsp', 'wdr', 'bool', 'bool2', 'index', 'ch4'])  # removed co2 from this list\n",
    "    else:\n",
    "        data = data.drop(columns=['x', 'y', 'wsp', 'wdr', 'bool', 'bool2', 'index'])\n",
    "    if ch4_bool is True:\n",
    "        data = data.rename(columns={'avg_wdr': 'wdr', 'avg_wsp': 'wsp', 'avg_ch4': 'ch4'})\n",
    "    else:\n",
    "        data = data.rename(columns={'avg_wdr': 'wdr', 'avg_wsp': 'wsp'})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radon_wind_pairing_func(met_df, radon_df):\n",
    "    \"\"\"\n",
    "    This function is meant to pair met and VOC data together (can also add methane data if needed)\n",
    "    The function averages the met data over the 10 minute GC VOC sampling interval and places the data point at the\n",
    "    end of the interval.  Wind direction data is averaged using a special method since it has a circular scale\n",
    "    (0 to 360).  Methane may also be included in the averaged interval if it is present in the met data.\n",
    "    Parameters\n",
    "    __________\n",
    "    met_df : object\n",
    "        dataframe containing met data\n",
    "    voc_df : object\n",
    "        dataframe containing VOC data\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        returns a dataframe with merged radon and met data\n",
    "    \"\"\"\n",
    "    # imput met df and voc df returns merged df with average windspeed and durection over voc sampaling interval\n",
    "    radon_df['time'] = radon_df['time'].dt.round('1min')\n",
    "    bool_list = met_df['time'].isin(radon_df['time'])\n",
    "    met_df['bool'] = bool_list\n",
    "    data = pd.merge(radon_df, met_df, on='time', how='outer')\n",
    "    data = data.set_index(['time'])  # sorting index by date\n",
    "    data = data.sort_index()\n",
    "    data = data.reset_index()\n",
    "    ind_bool = data.loc[data['bool'] == True]\n",
    "    goodind = ind_bool.index.tolist()\n",
    "    data['wdr'] = data['wdr'].astype(float)  # converting wdr and wsp into floating point values for later functions\n",
    "    data['wsp'] = data['wsp'].astype(float)\n",
    "    if 'wsp_avg_ms' in list(data.columns) or 'wdr_avg' in list(data.columns):\n",
    "        data = data.rename(columns={'wsp_avg_ms': 'wsp', 'wdr_avg': 'wdr'})\n",
    "    data['wdr_x'] = np.sin((data['wdr'] * np.pi / 180))  # converting wdr into east-west and north-south components\n",
    "    data['wdr_y'] = np.cos((data['wdr'] * np.pi / 180))\n",
    "\n",
    "    # creating list of lists with each nested list comprised of index values for averaging\n",
    "    avg_inds_list = []\n",
    "    for ind in goodind:\n",
    "        num = 0\n",
    "        ind_buffer_list = []\n",
    "        while num < 10:\n",
    "            ind_buffer_list.append(ind-num)\n",
    "            num += 1\n",
    "        avg_inds_list.append(ind_buffer_list)\n",
    "\n",
    "    # grabbing relivant wsp, wdr, and time data to do averaging\n",
    "    avg_wsp_list = []\n",
    "    avg_wdr_list = []\n",
    "    time_interval_list = []\n",
    "    for lst in avg_inds_list:\n",
    "        wdr_x_list = []\n",
    "        wdr_y_list = []\n",
    "        wsp_list = []\n",
    "        for ind in lst:\n",
    "            wdr_x_list.append(data.iloc[ind]['wdr_x'])\n",
    "            wdr_y_list.append(data.iloc[ind]['wdr_y'])\n",
    "            wsp_list.append(data.iloc[ind]['wsp'])\n",
    "            if ind == lst[0]:\n",
    "                time_interval_list.append(data.iloc[ind]['time'])\n",
    "        avg_wsp_list.append(mean(wsp_list))\n",
    "        # getting avg wsp for each interval_group\n",
    "\n",
    "        # calculating wdr average for each group\n",
    "        wdr_x_tot = sum(wdr_x_list)\n",
    "        wdr_y_tot = sum(wdr_y_list)\n",
    "        dev = (wdr_x_tot / wdr_y_tot)\n",
    "        ark = (np.arctan(dev))\n",
    "        avg = (ark * 57.2958)\n",
    "        if wdr_x_tot > 0 and wdr_y_tot > 0:  # if loop used to determin which quadrent wind direction data fit in\n",
    "            avg_wdr_list.append(avg)\n",
    "        elif wdr_x_tot > 0 and wdr_y_tot < 0:\n",
    "            avg2nd = (avg + 180)\n",
    "            avg_wdr_list.append(avg2nd)\n",
    "        elif wdr_x_tot < 0 and wdr_y_tot < 0:\n",
    "            avg3rd = (avg + 180)\n",
    "            avg_wdr_list.append(avg3rd)\n",
    "        elif wdr_x_tot < 0 and wdr_y_tot > 0:\n",
    "            avg4th = (avg + 360)\n",
    "            avg_wdr_list.append(avg4th)\n",
    "        else:\n",
    "            avg_wdr_list.append(np.nan)\n",
    "\n",
    "    # created a combine df and merging it with the radon df\n",
    "    combine_df = pd.DataFrame()\n",
    "    combine_df['wdr'] = avg_wdr_list\n",
    "    combine_df['wsp'] = avg_wsp_list\n",
    "    combine_df['time'] = time_interval_list\n",
    "    combine_df = pd.merge(combine_df, radon_df, on='time', how='outer')\n",
    "    return combine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_methane_combine_func(data_parameters, wind_list, methane_list):\n",
    "    \"\"\"\n",
    "    combines met and methane data into a single df then adds df to list of df(s).\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_parameters : object\n",
    "        class object storing parameters and constants for making windrose ready files.\n",
    "    wind_list : list of objects\n",
    "        list of met df's (1 df per site being run)\n",
    "    methane_list : list of objects\n",
    "        list of met df's (1 df per site being run)\n",
    "    Returns\n",
    "    --------\n",
    "    list of objects\n",
    "        list of combine met and methane df's\n",
    "    \"\"\"\n",
    "    met_list = []\n",
    "    for i in range(len(data_parameters.sites)):\n",
    "        combine_data = pd.merge(wind_list[i], methane_list[i], on='time', how='outer')\n",
    "        met_list.append(combine_data)\n",
    "    return met_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_methane_voc_combine_func(data_parameters, wind_list, data_list):\n",
    "    \"\"\"\n",
    "    met and methane data are already combine and then combine voc data with that. (returns list with combined df's)\n",
    "    uses voc_wind_pairing_func to combine voc's with met and methane data on VOC sampling interval.\n",
    "    adds benz_tol, pro_eth, in_pent, in_bute, and eth_meth columns.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_parameters : object\n",
    "        class object storing parameters and constants for making windrose ready files\n",
    "    wind_list : list of objects\n",
    "        list of met df's\n",
    "    data_list : list of objects\n",
    "        list of VOC df's\n",
    "    Returns\n",
    "    --------\n",
    "    list of objects\n",
    "        list of combine data frames with voc's, methane, and met data\n",
    "    \"\"\"\n",
    "    combine_data = []\n",
    "    for i in range(len(data_parameters.sites)):  # makeing voc ratio columns\n",
    "        data_list[i]['benz_tol'] = data_list[i]['benzene'] / data_list[i]['toluene']\n",
    "        data_list[i]['pro_eth'] = data_list[i]['propane'] / data_list[i]['ethane']\n",
    "        data_list[i]['in_pent'] = data_list[i]['i-pentane'] / data_list[i]['n-pentane']\n",
    "        data_list[i]['in_bute'] = data_list[i]['i-butane'] / data_list[i]['n-butane']\n",
    "        data = voc_wind_pairing_func(wind_list[i], data_list[i])\n",
    "        data['eth_meth'] = (data['ethane'] / data['ch4'])\n",
    "        combine_data.append(data)\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_voc_combine_func(data_parameters, wind_list, data_list):\n",
    "    \"\"\"\n",
    "    combine voc data with met data (returns list with combened df's)\n",
    "    uses voc_wind_pairing_func to combine voc's with met data on VOC sampling interval.\n",
    "    adds benz_tol, pro_eth, in_pent, and in_bute  columns.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_parameters : object\n",
    "        class object storing parameters and constants for making windrose ready files\n",
    "    wind_list : list of objects\n",
    "        list of met df's\n",
    "    data_list : list of objects\n",
    "        list of VOC df's\n",
    "    Returns\n",
    "    --------\n",
    "    list of objects\n",
    "        list of combine data frames with voc's and met data\n",
    "    \"\"\"\n",
    "    combine_data = []\n",
    "    for i in range(len(data_parameters.sites)):  # makeing voc ratio columns\n",
    "        data_list[i]['benz_tol'] = data_list[i]['benzene'] / data_list[i]['toluene']\n",
    "        data_list[i]['pro_eth'] = data_list[i]['propane'] / data_list[i]['ethane']\n",
    "        data_list[i]['in_pent'] = data_list[i]['i-pentane'] / data_list[i]['n-pentane']\n",
    "        data_list[i]['in_bute'] = data_list[i]['i-butane'] / data_list[i]['n-butane']\n",
    "        data = voc_wind_pairing_func(wind_list[i], data_list[i])\n",
    "        combine_data.append(data)\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_radon_combine_func(data_parameters, wind_list, data_list):\n",
    "    \"\"\"\n",
    "    combine voc data with met data (returns list with combened df's)\n",
    "    uses voc_wind_pairing_func to combine voc's with met data on VOC sampling interval.\n",
    "    adds benz_tol, pro_eth, in_pent, and in_bute  columns.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_parameters : object\n",
    "        class object storing parameters and constants for making windrose ready files\n",
    "    wind_list : list of objects\n",
    "        list of met df's\n",
    "    data_list : list of objects\n",
    "        list of VOC df's\n",
    "    Returns\n",
    "    --------\n",
    "    list of objects\n",
    "        list of combine data frames with voc's and met data\n",
    "    \"\"\"\n",
    "    combine_data = []\n",
    "    for i in range(len(data_parameters.sites)):  # makeing voc ratio columns\n",
    "        data = radon_wind_pairing_func(wind_list[i], data_list[i])\n",
    "        combine_data.append(data)\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_non_voc_combine_func(data_parameters, wind_list, data_list):\n",
    "    \"\"\"\n",
    "    combine non-voc data with met data (returns list with combened df's)\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_parameters : object\n",
    "        class object storing parameters and constants for making windrose ready files\n",
    "    wind_list : list of objects\n",
    "        list of met df's\n",
    "    data_list : list of objects\n",
    "        list of non-VOC df's\n",
    "    Returns\n",
    "    --------\n",
    "    list of objects\n",
    "        list of combine data frames with non-voc and met data\n",
    "    \"\"\"\n",
    "    combine_data = []\n",
    "    for i in range(len(data_parameters.sites)):  # makeing voc ratio columns\n",
    "        if 'wdr' in data_list[i].columns or 'wdr_avg' in data_list[i].columns:\n",
    "            data = wind_list[i]\n",
    "            combine_data.append(data)\n",
    "        else:\n",
    "            data = pd.merge(data_list[i], wind_list[i], on='time', how='outer')\n",
    "            combine_data.append(data)\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_filter_func(combine_data):\n",
    "    \"\"\"\n",
    "    Used to replace all 0 and negative values in a dataframe with NaN's\n",
    "    Parameters\n",
    "    __________\n",
    "    combine_data : list of objects\n",
    "        list of dataframes you want to replace 0 and negative vales in.\n",
    "    Returns\n",
    "    ________\n",
    "    list of objects\n",
    "         returns list of df's with 0 and negative values replaced with NaN's\n",
    "    \"\"\"\n",
    "    filtered_combine_data = []\n",
    "    for df in combine_data:\n",
    "        df = df.set_index(df['time'])\n",
    "        df = df.drop(columns=['time'])\n",
    "        df = df.mask(df <= 0)\n",
    "        df = df.reset_index()\n",
    "        filtered_combine_data.append(df)\n",
    "    return filtered_combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_zero_filter_func(combine_data):\n",
    "    \"\"\"\n",
    "    Used to replace all 0 and negative values in a dataframe with NaN's\n",
    "    Parameters\n",
    "    __________\n",
    "    combine_data : objects\n",
    "        a combine site dataframes you want to replace 0 and negative vales in.\n",
    "    Returns\n",
    "    ________\n",
    "    object\n",
    "         returns df with 0 and negative values replaced with NaN's\n",
    "    \"\"\"\n",
    "    for col in combine_data.columns:\n",
    "        try:\n",
    "            combine_data[col] = combine_data[col].mask(combine_data[col] <= 0)\n",
    "        except:\n",
    "            continue\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_ready_export_func(data_parameters, combine_data):\n",
    "    \"\"\"\n",
    "    Used to export R OpenAir ready data\n",
    "    Parameters\n",
    "    __________\n",
    "    data_parameters : object\n",
    "        class object storing parameters and constants for making windrose ready files\n",
    "    combine_data : list of objects\n",
    "        data frame containing combined met and species data ready for export\n",
    "    Returns\n",
    "    ________\n",
    "    None\n",
    "         exports the data to out directory specified in the data_parameters object.  With wsp data removed above filter\n",
    "         value\n",
    "    \"\"\"\n",
    "    num = 0\n",
    "    for df in combine_data:\n",
    "        df = df.loc[df['wsp'] > data_parameters.wsp_filter]\n",
    "        df.to_csv(data_parameters.export_dir + '\\\\' + data_parameters.sites[num] + '_' +\n",
    "                  data_parameters.species + '_' + data_parameters.start_time[:10] + '__' +\n",
    "                  data_parameters.end_time[:10] + '_Wind_Plot_Ready.csv', index=False, encoding='utf-8')\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_loc_func(quarter_year):\n",
    "    \"\"\"\n",
    "    this function makes a list of all relivant quarters and years between your start_time and end_time it also returns\n",
    "    the quarter for your title\n",
    "    Parameters\n",
    "    __________\n",
    "    quarter_year : list of str\n",
    "        list of all quarter year strings inside user specified start and end dates.\n",
    "    Returns\n",
    "    ________\n",
    "    list of ints, str\n",
    "        returns list of wanted months for the quarter you are using and a title string of that quarter\n",
    "    \"\"\"\n",
    "    quarter = quarter_year[:2]\n",
    "    if quarter == 'Q1':\n",
    "        good_months = [1, 2, 3]\n",
    "    elif quarter == 'Q2':\n",
    "        good_months = [4, 5, 6]\n",
    "    elif quarter == 'Q3':\n",
    "        good_months = [7, 8, 9]\n",
    "    else:\n",
    "        good_months = [10, 11, 12]\n",
    "    return good_months, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_column_correction_func(wind_list):\n",
    "    \"\"\"\n",
    "    this function replaces 'wsp_avg_ms' & 'wdr_avg' headers with 'wsp' & 'wdr' for all df's in a list of df's\n",
    "    Parameters\n",
    "    __________\n",
    "    wind_list : list of objects\n",
    "        list of all met df's imported\n",
    "    Returns\n",
    "    ________\n",
    "    list of objects\n",
    "        returns list of df's with unwanted header names replaced\n",
    "    \"\"\"\n",
    "    header_correction_list = []\n",
    "    for df in wind_list:\n",
    "        if 'wsp_avg_ms' in df.columns and 'wdr_avg' in df.columns:\n",
    "            df = df.rename(columns={\"wsp_avg_ms\": \"wsp\", \"wdr_avg\": \"wdr\"})\n",
    "            header_correction_list.append(df)\n",
    "        else:\n",
    "            header_correction_list.append(df)\n",
    "    return header_correction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_column_func(combine_data, data_parameters):\n",
    "    \"\"\"\n",
    "    this function adds latitude \"lat\" and longitude \"lon\" columns to each df based on site\n",
    "    Parameters\n",
    "    __________\n",
    "    combine_data : list of objects\n",
    "        list of all combine df's (one for each site)\n",
    "    data_parameters : object\n",
    "        class object storing parameters and constants for making windrose ready files\n",
    "    Returns\n",
    "    ________\n",
    "    list of objects\n",
    "        returns list of df's with lat and lon columns added specific to each site's combine df\n",
    "    \"\"\"\n",
    "    for i in range(len(data_parameters.sites)):\n",
    "        combine_data[i]['lat'] = LAT_LON_DICT.get(data_parameters.sites[i])[0]\n",
    "        combine_data[i]['lon'] = LAT_LON_DICT.get(data_parameters.sites[i])[1]\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
